{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58dad77b",
   "metadata": {},
   "source": [
    "## Import Datasets\n",
    "\n",
    "**Make sure that you've downloaded the required human and dog datasets:\n",
    "Download the dog dataset. Unzip the folder and place it in this project's home directory, at the location /dog_images.\n",
    "Download the human dataset. Unzip the folder and place it in the home directory, at location /lfw.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c0c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip dogImages.zip\n",
    "#!unzip lfw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# load filenames for human and dog images\n",
    "human_files = np.array(glob(\"./lfw/*/*\"))\n",
    "dog_files = np.array(glob(\"./dogImages/*/*/*\"))\n",
    "\n",
    "# print number of images in each dataset\n",
    "print('There are %d total human images.' % len(human_files))\n",
    "print('There are %d total dog images.' % len(dog_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ce76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(dog_files[1])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484d6799",
   "metadata": {},
   "source": [
    "## Obtain Pre-trained VGG-16 Model\n",
    "**The code cell below downloads the VGG-16 model, along with weights that have been trained on ImageNet, a very large, very popular dataset used for image classification and other vision tasks. ImageNet contains over 10 million URLs, each linking to an image containing an object from one of 1000 categories.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e80074",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e914a897",
   "metadata": {},
   "source": [
    "First, we can use the load_img() function to load the image and resize it to the required size of 224Ã—224 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dddf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "image = load_img('./dogImages/train/001.Affenpinscher/Affenpinscher_00002.jpg',target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04642059",
   "metadata": {},
   "source": [
    "Next, we can convert the pixels to a NumPy array so that we can work with it in Keras. We can use the img_to_array() function for this. We only have one sample (one image). We can reshape the array by calling reshape() and adding the extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52b547",
   "metadata": {},
   "source": [
    "Keras provides a function called preprocess_input() to prepare new input for the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219a501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7472f43",
   "metadata": {},
   "source": [
    "We are now ready to make a prediction for our loaded and prepared image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45229fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10692f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
